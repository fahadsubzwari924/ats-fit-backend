# Local LLM Configuration
# Copy this to your .env file and adjust values as needed

# Ollama service URL
LOCAL_LLM_OLLAMA_URL=http://localhost:11434

# Model name to use (make sure it's installed in Ollama)
# Common options: llama3, llama3.1, phi3, codellama, mistral, etc.
# To see available models, run: ollama list
LOCAL_LLM_MODEL_NAME=llama3

# Alternative models you can try:
# LOCAL_LLM_MODEL_NAME=llama3.1
# LOCAL_LLM_MODEL_NAME=phi3
# LOCAL_LLM_MODEL_NAME=mistral

# To install a model in Ollama, run:
# ollama pull llama3
# ollama pull llama3.1
# ollama pull phi3
